{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-Hxs7WSa21mR",
        "99y9QnJveipi",
        "vI3BEAaYf08W",
        "Y2XVUYVue_aM"
      ],
      "mount_file_id": "1wWWtTJoNx1mLRFxYPPwUCVXGiFCUShgW",
      "authorship_tag": "ABX9TyO0GQk1qmLIbsvZpk0RP0HZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patshandofdoom/SECFindings/blob/main/SECScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a"
      ],
      "metadata": {
        "id": "xzD5o3m6gSlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference Data\n",
        "\n",
        "[This is an edgar parser](https://github.com/rsljr/edgarParser). It can pull info from forms to show you what was written within\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nhniH0zMToRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establish environment"
      ],
      "metadata": {
        "id": "p8peHtWBPk-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/69025255/how-to-web-scraping-sec-edgar-10-k-dynamic-data\n",
        "#removed the install of fake-useragent and import useragent from fake-useragent\n",
        "!pip --quiet install reticker lxml google -U selenium\n",
        "!pip install --quiet mysqlclient mysql-connector-python pandas\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import sys\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import MySQLdb\n",
        "import json\n",
        "import mysql.connector\n",
        "from mysql.connector import Error\n"
      ],
      "metadata": {
        "id": "M9yUcPU1BpqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0da994-f2e1-4720-fd31-907014b06035"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/89.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for mysqlclient (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find New opportunities via the Benzinga live feed page\n"
      ],
      "metadata": {
        "id": "1D2AB6BoLiuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open Benzinga web page, copy table and store info into databaasetable\n",
        "\n",
        "---\n",
        "\n",
        "Need to store items that have already been sent using the \"Ticker\" and \"Date Announced\" in a seperate file. Any new item should send a notification\n"
      ],
      "metadata": {
        "id": "v_dSfJN3LrWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#all required packages\n",
        "from datetime import date\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "#reticker makes it easier to regex tickers from a string\n",
        "import reticker\n",
        "extractor = reticker.TickerExtractor()\n",
        "\n",
        "#set up the database credentials for each of the functions\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/SEC Scraping/Scraping.txt','r') as readfile:\n",
        "  credentials = readfile.read()\n",
        "  credentials = json.loads(credentials)\n",
        "\n",
        "dbhost = credentials[\"HOST\"]\n",
        "dbuser = credentials[\"USERNAME\"]\n",
        "dbpasswd= credentials[\"PASSWORD\"]\n",
        "dbname = credentials[\"DATABASE\"]\n",
        "\n",
        "##########################################################################################\n",
        "#if day and month have <2 characters, add a 0\n",
        "def adddigit(characters):\n",
        "  if (len(characters) < 2):\n",
        "    return( \"0\"+characters)\n",
        "  else:\n",
        "    return(characters)\n",
        "\n",
        "###############################################################################\n",
        "  #This section builds the HTML code using today's date onward and fetches the response\n",
        "\n",
        "def URLsoupFind():\n",
        "  #get todays date\n",
        "  today = date.today()\n",
        "  startyear = str(today.year)\n",
        "  startmonth = adddigit(str(today.month))\n",
        "  startday = adddigit(str(today.day))\n",
        "\n",
        "  #get date 2 months out\n",
        "  enddate = date.today() + relativedelta(months=+2)\n",
        "  endyear = str(enddate.year)\n",
        "  endmonth = adddigit(str(enddate.month))\n",
        "  endday = adddigit(str(enddate.day))\n",
        "\n",
        "\n",
        "  base_url = f\"https://www.benzinga.com/calendars/stock-splits?date_from={startyear}-{startmonth}-{startday}&date_sort=ex&date_to={endyear}-{endmonth}-{endday}\"\n",
        "  theseheaders = {\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
        "  print(base_url)\n",
        "  benzingaresponse = requests.get(base_url,headers=theseheaders)\n",
        "\n",
        "  benzingasoup = BeautifulSoup(benzingaresponse.text)\n",
        "  return benzingasoup\n",
        "\n",
        "###############################################################################\n",
        "#this segment checks live table on the benzinga website\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#this segment checks the live feed on the benzinga website\n",
        "def shortfeed(benzingasoup):\n",
        "  mydivs = benzingasoup.findAll('div',class_=\"mb-4\")\n",
        "  #pull out all of the 'a' divisions\n",
        "  allupdates = mydivs[0].findAll('a')\n",
        "  finalarray = []\n",
        "\n",
        "  for updates in allupdates:\n",
        "    #find all the spans in 'a' and make them into an array\n",
        "    updatetitlespans = updates.findAll('span')\n",
        "\n",
        "    #Sometimes the 'a' segments will have 3 spans, others will have more.\n",
        "    #If it has 3 spans, we can grab the info from span 3 for the\n",
        "    if(len(updatetitlespans)>3):\n",
        "      stockticker = re.findall(\"\\:(.*?)\\)\",updatetitlespans[2].text)\n",
        "      #ran into an issue where one of the parentheses included 2 tickers. so I added a clause here to split those adn take the first.\n",
        "      #Need to make sure the stockticker has length, then split it on the comma and take the first\n",
        "      if len(stockticker) >0:\n",
        "        if \",\" in stockticker[0]:\n",
        "            stockticker1 = re.split(\",\",stockticker[0])\n",
        "            stockticker = [stockticker1[0]]\n",
        "\n",
        "      instancetext = updatetitlespans[2].text\n",
        "\n",
        "    #Otherwise, we grab the story text from span 2 and see if span 3 has any info about the ticker\n",
        "    else:\n",
        "      stockticker = re.findall(\"\\:(.*?)\\)\",updatetitlespans[2].text)\n",
        "      #ran into an issue where one of the parentheses included 2 tickers. so I added a clause here to split those adn take the first.\n",
        "      #Need to make sure the stockticker has length, then split it on the comma and take the first\n",
        "      if len(stockticker) >0:\n",
        "        if \",\" in stockticker[0]:\n",
        "            stockticker1 = re.split(\",\",stockticker[0])\n",
        "            print(stockticker1)\n",
        "            stockticker = [stockticker1[0]]\n",
        "\n",
        "      instancetext = updatetitlespans[1].text\n",
        "      instancetext = instancetext.replace('\"','')\n",
        "    #If we cant find the ticker, try using extractor\n",
        "    if len(stockticker) == 0:\n",
        "      stockticker = extractor.extract(updatetitlespans[2].text)\n",
        "    #If it still cant be found, make it N/A\n",
        "    if len(stockticker) == 0:\n",
        "      stockticker = \"N/A\"\n",
        "    else:\n",
        "      #finally, remove all space caharcters from the ticker\n",
        "      stockticker = stockticker[0].replace(u'\\xa0', '')\n",
        "      stockticker = stockticker.replace(' ', '')\n",
        "    #The split title will be in the second span\n",
        "    instancetitle = updatetitlespans[1].text\n",
        "    #replace all quotation marks in the title adn text with apostrophes\n",
        "    instancetitle = instancetitle.replace('\"',\"'\")\n",
        "    instancetext = instancetext.replace('\"',\"'\")\n",
        "\n",
        "    instancedict = {'ticker':stockticker,\n",
        "                 'title':instancetitle,\n",
        "                 'articletext':instancetext}\n",
        "    finalarray.append(instancedict)\n",
        "  return finalarray\n",
        "\n",
        "###############################################################################\n",
        "#this function creates the general google search that I usually do by hand for each of these tickers\n",
        "#it returns the HTML link of the first result of that search\n",
        "def searchresults(ticker):\n",
        "  try:\n",
        "      from googlesearch import search\n",
        "  except ImportError:\n",
        "      print(\"No module named 'google' found\")\n",
        "\n",
        "  # to search\n",
        "  query = 'reverse split fractional \"'+ticker+'\" site:yahoo.com'\n",
        "\n",
        "  for j in search(query, tld=\"co.in\", num=10, stop=10, pause=2):\n",
        "      return(j)\n",
        "\n",
        "###############################################################################\n",
        "#next it need to keep a database of already sent items, then send the items to the discord bot\n",
        "#pass in a dictionary of columns to search and values to search for then pass out the result. If it is an array with no length, you are good. otherwise there is a matching entry\n",
        "def check_for_entries(whichtable):\n",
        "  #initiate Connection with the database\n",
        "  connection = mysql.connector.connect(host = dbhost, user = dbuser, passwd= dbpasswd, db = dbname, autocommit = True, )\n",
        "  #Try connecting and working with the database\n",
        "  try:\n",
        "    if connection.is_connected():\n",
        "        #Create a query which will return several rows of data that we can search\n",
        "        mySql_Create_Table_Query = \"SELECT max(id_number), title FROM \"+whichtable+\" GROUP BY id_number LIMIT 100\"\n",
        "        cursor = connection.cursor(buffered=True)\n",
        "        cursor.execute(mySql_Create_Table_Query)\n",
        "        result = cursor.fetchall()\n",
        "\n",
        "  except Error as e:\n",
        "      print(\"Error while connecting to MySQL\", e)\n",
        "  finally:\n",
        "      if connection.is_connected():\n",
        "          cursor.close()\n",
        "          connection.close()\n",
        "          print(\"Search connection is closed\")\n",
        "      return result\n",
        "\n",
        "###############################################################################\n",
        "#This function takes in a dictionary of the table values and compares it to the lates database entries\n",
        "def checkformatch(eachdictionary,latestdatabaseentries):\n",
        "  found = False\n",
        "  i=0\n",
        "\n",
        "  while found==False and  i<len(latestdatabaseentries):\n",
        "\n",
        "    if(latestdatabaseentries[i][1]==eachdictionary[\"title\"]):\n",
        "      found = True\n",
        "    i+=1\n",
        "  return found\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#pass in the dictionary that we are going to write to and which table we are writing to\n",
        "def write_entries(whatdictionary,whichtable):\n",
        "  #initiate Connection with the database\n",
        "  connection = mysql.connector.connect(host = dbhost, user = dbuser, passwd= dbpasswd, db = dbname, autocommit = True, )\n",
        "  #Try connecting and working with the database\n",
        "  try:\n",
        "    if connection.is_connected():\n",
        "        #build the query to search in SQL. This adds the search\n",
        "        databaseheaders=\"\"\n",
        "        databasevalues=\"\"\n",
        "        counter = 0\n",
        "        keys = whatdictionary.keys()\n",
        "        for each in keys:\n",
        "          databaseheaders+= each\n",
        "          databasevalues+='\"'+whatdictionary[each]+'\"'\n",
        "          counter+=1\n",
        "          if counter != len(keys):\n",
        "            databaseheaders+=\",\"\n",
        "            databasevalues+=\",\"\n",
        "\n",
        "        #Submit the addition\n",
        "        mySql_Create_Table_Query = \"insert into \"+whichtable+\"(\"+databaseheaders+\") VALUES(\"+databasevalues+\")\"\n",
        "        print(mySql_Create_Table_Query)\n",
        "        cursor = connection.cursor(buffered=True)\n",
        "        cursor.execute(mySql_Create_Table_Query)\n",
        "\n",
        "  except Error as e:\n",
        "      print(\"Error while connecting to MySQL\", e)\n",
        "  finally:\n",
        "      if connection.is_connected():\n",
        "          cursor.close()\n",
        "          connection.close()\n",
        "          print(\"Write connection is closed\")\n",
        "\n",
        "###############################################################################\n",
        "# Function that sends a message to the Discord channel\n",
        "def send_message(ticker, title, text, url):\n",
        "    WEBHOOK_URL = 'https://discord.com/api/webhooks/1102332754965831822/DOQYwrS8TAE8Es1EaS44A-z58aANgkwuXXTqcIslCS0IKTKm_l3Mt1Dr_xnlnpVHyKHd'\n",
        "    message = f'TICKER: {ticker}\\n TITLE: {title}\\n DETAILS: {text}\\n URL: {url}'\n",
        "    data = {'content': message}\n",
        "    requests.post(WEBHOOK_URL, json=data)\n",
        "\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "#Begin Process.\n",
        "#First grab the live table from benzinga, then grab the full table from benzinga\n",
        "#store each of the items from each table into their respective database tables\n",
        "\n",
        "#get the soup, get the live table and parse it.\n",
        "originalSoup = URLsoupFind()\n",
        "livetablefinalarray = shortfeed(originalSoup)\n",
        "latestdatabaseentries = check_for_entries('benzingaliveupdates')\n",
        "\n",
        "#for each of the values in the benzinga live table, check if its in the database already\n",
        "for eachdictionary in livetablefinalarray:\n",
        "  print(eachdictionary)\n",
        "  #check for a match in database data\n",
        "  matchfound = checkformatch(eachdictionary,latestdatabaseentries)\n",
        "\n",
        "  #If no match is found, do the google search for me add a url variable\n",
        "  if matchfound ==False:\n",
        "    #If it didnt find the ticker, just add a dummy variable for the URL\n",
        "    if eachdictionary['ticker'] == \"N/A\":\n",
        "      eachdictionary['url'] =\"\"\n",
        "    else:\n",
        "      #Otherwise do the google search and log the first result\n",
        "      ticker = eachdictionary['ticker']\n",
        "      searchURL = searchresults(ticker)\n",
        "      eachdictionary['url'] =searchURL\n",
        "    #Add the roundup value of ? since we do not know yet if it will round up\n",
        "    eachdictionary['roundup'] = \"?\"\n",
        "    #then write it to the database\n",
        "    write_entries(eachdictionary,\"benzingaliveupdates\")\n",
        "    #finally send a message\n",
        "    print(eachdictionary)\n",
        "    send_message(ticker, eachdictionary['title'], eachdictionary['articletext'],searchURL)\n",
        "\n",
        "\n",
        "  else:\n",
        "    print(\"This entry is already in the database\")"
      ],
      "metadata": {
        "id": "ziWYQlHpL2uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find new opportunities in the main benzinga table"
      ],
      "metadata": {
        "id": "-Hxs7WSa21mR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an attempt to access the main benzinga table. unfortunately its loaded via javascript so I will need to use selenium instead of just a get function."
      ],
      "metadata": {
        "id": "sLZhh8JY2_fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###https://saturncloud.io/blog/how-to-use-selenium-webdriver-in-google-colab/\n",
        "\n",
        "# this section will load the main table from the\n",
        "#all required packages\n",
        "from datetime import date\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import requests\n",
        "import re\n",
        "#reticker makes it easier to regex tickers from a string\n",
        "import reticker\n",
        "extractor = reticker.TickerExtractor()\n",
        "\n",
        "#if day and month have <2 characters, add a 0\n",
        "def adddigit(characters):\n",
        "  if (len(characters) < 2):\n",
        "    return( \"0\"+characters)\n",
        "  else:\n",
        "    return(characters)\n",
        "\n",
        "###############################################################################\n",
        "#This section builds the HTML code using today's date onward and fetches the response\n",
        "\n",
        "def URLsoupFind():\n",
        "  #get todays date\n",
        "  today = date.today()\n",
        "  startyear = str(today.year)\n",
        "  startmonth = adddigit(str(today.month))\n",
        "  startday = adddigit(str(today.day))\n",
        "\n",
        "  #get date 2 months out\n",
        "  enddate = date.today() + relativedelta(months=+2)\n",
        "  endyear = str(enddate.year)\n",
        "  endmonth = adddigit(str(enddate.month))\n",
        "  endday = adddigit(str(enddate.day))\n",
        "\n",
        "  base_url = f\"https://www.benzinga.com/calendars/stock-splits?date_from={startyear}-{startmonth}-{startday}&date_sort=ex&date_to={endyear}-{endmonth}-{endday}\"\n",
        "  #///////////////////////////////////\n",
        "  browser = webdriver.chrome.webdriver.WebDriver\n",
        "  browser.get(base_url)\n",
        "  html = browser.page_source\n",
        "  soup = BeautifulSoup(html, 'lxml')\n",
        "  a = soup.findAll('tbody')\n",
        "  #///////////////////////////////////\n",
        "\n",
        "  #theseheaders = {\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
        "  #print(base_url)\n",
        "  #benzingaresponse = requests.get(base_url,headers=theseheaders)\n",
        "\n",
        "  benzingasoup = BeautifulSoup(a.text)\n",
        "  return benzingasoup\n",
        "\n",
        "########################################################\n",
        "benzingasoup = URLsoupFind()\n",
        "print(benzingasoup)\n",
        "tables = benzingasoup.findAll('tr')\n",
        "print(tables)\n",
        "#pull out all of the 'a' divisions\n",
        "tablerows = tables[0].findAll('tr')\n",
        "print(tablerows)\n",
        "finalarray = []\n",
        "\n",
        "for row in tablerows:\n",
        "  #find all the spans in 'a' and make them into an array\n",
        "  tabledata = allupdates.findAll('td')\n",
        "  for datum in tabledata:\n",
        "    print(datum)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "9Eyl7Wk576sH",
        "outputId": "8dec63e8-11af-4f4c-cd52-6aa6a2892c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9466a66bef79>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m########################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mbenzingasoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mURLsoupFind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenzingasoup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbenzingasoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-9466a66bef79>\u001b[0m in \u001b[0;36mURLsoupFind\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;31m#///////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchrome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWebDriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: WebDriver.get() missing 1 required positional argument: 'url'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the files that we need to search on SEC RSS Feed"
      ],
      "metadata": {
        "id": "99y9QnJveipi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#establish environment\n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "#Log the url we need for the RSS feed of 8K documents from he SEC\n",
        "thisurl = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&CIK=&type=8&company=&dateb=&owner=include&start=0&count=40&output=atom\"\n",
        "#Create the headers to use on each of the html requests\n",
        "theseheaders = {\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
        "#loads the CIK data as a dataframe so that we dont need to load it a million times\n",
        "cikdf = pd.read_csv(\"https://www.sec.gov/include/ticker.txt\", delimiter='\\t', header=None)\n",
        "cikdf = cikdf.set_index(1)\n",
        "\n",
        "##########################################################################################\n",
        "#https://punit-arani.medium.com/get-the-cik-number-for-a-stock-using-python-2eaa494fbd91\n",
        "#This function will take a CIK number and translate it into a ticker.\n",
        "def gettickerfromCIK(ciknumber):\n",
        "    #return ticker if it exists\n",
        "    try:\n",
        "      ticker = cikdf.loc[int(ciknumber), 0]\n",
        "      if isinstance(ticker,pd.Series):\n",
        "        ticker = ticker.iloc[0]\n",
        "      print(ticker)\n",
        "      return ticker\n",
        "\n",
        "    #return None there's no ticker for the CIK\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "##########################################################################################\n",
        "#This function will take the SEC 8k filing URL and search for the top 40 newest 8K filings, returning a matrix of dictionaries\n",
        "def retrieve8klist():\n",
        "  edgar_resp = requests.get(thisurl,headers=theseheaders)\n",
        "  thissoup = edgar_resp.text\n",
        "  #important to have a list of filings\n",
        "  formlist = []\n",
        "  root = ET.fromstring(thissoup)\n",
        "  #for all child elements check if it's an entry\n",
        "  for child in root:\n",
        "    if child.tag ==\"{http://www.w3.org/2005/Atom}entry\":\n",
        "      #Then grab the title and href from each entry.\n",
        "      for nestedchild in child:\n",
        "        if nestedchild.tag == \"{http://www.w3.org/2005/Atom}title\":\n",
        "          fulltitle = nestedchild.text\n",
        "          formtype = fulltitle.split(\" - \")\n",
        "          formtype = formtype[0]\n",
        "        if nestedchild.tag == \"{http://www.w3.org/2005/Atom}link\":\n",
        "          formlink = nestedchild.attrib['href']\n",
        "      childdict = {'formtype':formtype,'fulltitle':fulltitle, 'link':formlink}\n",
        "      formlist.append(childdict)\n",
        "\n",
        "\n",
        "  return(formlist)\n",
        "\n",
        "##########################################################################################\n",
        "\n",
        "#this function will get the direct link to the 8K form\n",
        "def getformlinkfromfilinglink(filinglink):\n",
        "  #open the link and get the soup\n",
        "  edgar_resp = requests.get(filinglink,headers=theseheaders)\n",
        "  filingsoup = BeautifulSoup(edgar_resp.text)\n",
        "\n",
        "  #this segment of code finds the CIK number and grabs the ticker\n",
        "  companyinfo = filingsoup.find('span', class_='companyName')\n",
        "  ciknumber = companyinfo.find('a').text\n",
        "  ciknumber = re.split(\" \\([^)]*\\)\",ciknumber)\n",
        "  ciknumber = ciknumber[0]\n",
        "  ticker = gettickerfromCIK(ciknumber)\n",
        "\n",
        "  #from the soup, we get the table\n",
        "  tableentries = filingsoup.findAll('table',class_='tableFile')\n",
        "  linktoform = \"\"\n",
        "  #search through the rows for all data\n",
        "  for eachrow in tableentries[0].findAll('tr'):\n",
        "    celldata = eachrow.findAll('td')\n",
        "    #As long as this row has tr labels and the type is an 8-k form, get the link to the 8-k\n",
        "    if len(celldata)>0:\n",
        "      if celldata[3].text=='8-K':\n",
        "        for link in celldata[2].findAll('a',href=True):\n",
        "          link= link['href'].replace(\"ix?doc=\",\"\")\n",
        "          linktoform = \"https://sec.gov\"+link\n",
        "\n",
        "  return({'cik':ciknumber,'ticker':ticker,'formlink':linktoform})\n",
        "\n",
        "##########################################################################################\n",
        "#This takes the list from the Retrieve8k list and opens each link, parsing the internal data and returning the count\n",
        "def parse8klist(entry):\n",
        "  #this block here to open the filing page and get the right link from the table\n",
        "  if(entry['formtype']=='8-K'):\n",
        "    print(entry['link'])\n",
        "    #This function creates a dictionary which we can pass up to the database after we get all info\n",
        "    filinginformation = getformlinkfromfilinglink(entry['link'])\n",
        "    print(filinginformation['formlink'])\n",
        "\n",
        "    #find the filing URL in the webpage\n",
        "    edgar_resp = requests.get(filinginformation['formlink'],headers=theseheaders)\n",
        "\n",
        "    #Access the Filing\n",
        "    formsoup = edgar_resp.text\n",
        "    formsoup = formsoup.lower()\n",
        "\n",
        "    #find all instances of word reverse and word merger\n",
        "    revCount = re.findall(r'reverse',formsoup,re.MULTILINE)\n",
        "    mergercount = re.findall(r'merger',formsoup,re.MULTILINE)\n",
        "    #reverse count - count the number of times Reverse is in the text\n",
        "    print(len(revCount))\n",
        "\n",
        "    #merger count\n",
        "    print(len(mergercount))\n",
        "    time.sleep(1)\n",
        "    return([])\n",
        "\n",
        "##########################################################################################\n",
        "##########################################################################################\n",
        "##########################################################################################\n",
        "#Retrieve list of the 40 latest 8k forms filed\n",
        "rssof8kforms = retrieve8klist()\n",
        "#need a function here to get the\n",
        "\n",
        "#for each of those forms,\n",
        "for eachform in rssof8kforms:\n",
        "  parse8klist(eachform)\n",
        "\n",
        "\n",
        "ticker = gettickerfromCIK()\n"
      ],
      "metadata": {
        "id": "Nol-57z6eg4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "outputId": "7d3caa9f-244b-4d6c-b798-1f29aa2ff4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.sec.gov/Archives/edgar/data/1269371/000106823823000157/0001068238-23-000157-index.htm\n",
            "0001269371\n",
            "https://sec.gov/Archives/edgar/data/1269371/000106823823000157/sntlt2003-5_8k.htm\n",
            "0\n",
            "0\n",
            "https://www.sec.gov/Archives/edgar/data/1264157/000106823823000156/0001068238-23-000156-index.htm\n",
            "0001264157\n",
            "https://sec.gov/Archives/edgar/data/1264157/000106823823000156/sntlt2003-4_8k.htm\n",
            "0\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0bba84b0363d>\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m#for each of those forms,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meachform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrssof8kforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0mparse8klist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meachform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0bba84b0363d>\u001b[0m in \u001b[0;36mparse8klist\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m#merger count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmergercount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block is a snippet which will grab data from a filing URL and parse it for how many mentions it contains to reverse or merger.\n",
        "\n",
        "The code in this block is here to test expanding the parse8klist() function above"
      ],
      "metadata": {
        "id": "mMHoGMFWuWk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#headers = ua.random\n",
        "headers = {\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
        "\n",
        "# Access page\n",
        "cik = '200406'\n",
        "type = '10-K'\n",
        "dateb = '20210704'\n",
        "\n",
        "# Obtain HTML for search page\n",
        "#base_url = f\"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK={cik}&type={type}&dateb={dateb}\"\n",
        "base_url = 'ix?doc=/Archives/edgar/data/101984/000010198423000034/ueic-20230605.htm'\n",
        "\n",
        "#have to split off the ix?doc bullcrap, get righ to the HTM link\n",
        "link= base_url.split(\"ix?doc=\")\n",
        "base_url = 'https://www.sec.gov'+link[1]\n",
        "edgar_resp = requests.get(base_url,headers=headers)\n",
        "\n",
        "thissoup = edgar_resp.text\n",
        "thissoup = thissoup.lower()\n",
        "\n",
        "#reverse count - count the number of times Reverse is in the text\n",
        "print(len(re.findall(r'reverse',thissoup,re.MULTILINE)))\n",
        "\n",
        "#merger count\n",
        "print(len(re.findall(r'universal',thissoup,re.MULTILINE)))\n"
      ],
      "metadata": {
        "id": "XsmOZnnLYBPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1633ff4b-29be-45b7-a2ef-4c4579424ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code below is to try and find the CIP list number in order to grab the ticker"
      ],
      "metadata": {
        "id": "_40A9Xv8KbcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#loads the CIK data as a dataframe so that we dont need to load it a million times\n",
        "cikdf = pd.read_csv(\"https://www.sec.gov/include/ticker.txt\", delimiter='\\t', header=None)\n",
        "cikdf = cikdf.set_index(1)\n",
        "def gettickerfromCIK(ciknumber):\n",
        "    #return ticker if it exists\n",
        "    try:\n",
        "      ticker = cikdf.loc[int(ciknumber), 0]\n",
        "      if isinstance(ticker,pd.Series):\n",
        "        ticker = ticker.iloc[0]\n",
        "      return ticker\n",
        "    #return None there's no ticker for the CIK\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "###############################################################################\n",
        "headers = {\"User-agent\":\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36\"}\n",
        "url = \"https://www.sec.gov/Archives/edgar/data/1042776/000119312523193273/0001193125-23-193273-index.htm\"\n",
        "edgar_resp = requests.get(url,headers=headers)\n",
        "thissoup = BeautifulSoup(edgar_resp.text)\n",
        "\n",
        "companyinfo = thissoup.find('span', class_='companyName')\n",
        "ciknumber = companyinfo.find('a').text\n",
        "\n",
        "ciknumber = re.split(\" \\([^)]*\\)\",ciknumber)\n",
        "ciknumber = ciknumber[0]\n",
        "\n",
        "print(ciknumber)\n",
        "\n",
        "ticker = gettickerfromCIK(ciknumber)\n",
        "\n",
        "print(ticker)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np3XQ4THKboS",
        "outputId": "38a00346-d307-4e8c-ffec-16b656599f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0001042776\n",
            "pdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Access Database\n"
      ],
      "metadata": {
        "id": "k4JYHSx7JGcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a segment will attach to the SQL databsae and read data"
      ],
      "metadata": {
        "id": "NLsjL3B7PdYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Open credentials file and load the JSON variables inside\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/SEC Scraping/Scraping.txt','r') as readfile:\n",
        "  credentials = readfile.read()\n",
        "  credentials = json.loads(credentials)\n",
        "\n",
        "#initiate Connection with the database\n",
        "connection = mysql.connector.connect(\n",
        "  host = credentials[\"HOST\"],\n",
        "  user = credentials[\"USERNAME\"],\n",
        "  passwd= credentials[\"PASSWORD\"],\n",
        "  db = credentials[\"DATABASE\"],\n",
        "  autocommit = True,\n",
        ")\n",
        "\n",
        "#Try connecting and working with the database\n",
        "try:\n",
        "  if connection.is_connected():\n",
        "      db_Info = connection.get_server_info()\n",
        "      print(\"Connected to MySQL Server version \", db_Info)\n",
        "      cursor = connection.cursor(buffered=True)\n",
        "      cursor.execute(\"select database();\")\n",
        "      record = cursor.fetchone()\n",
        "      print(\"You're connected to database: \", record)\n",
        "\n",
        "      ##this section is where the query code will go\n",
        "      mySql_Create_Table_Query = \"SELECT * FROM benzingaliveupdates\"\n",
        "\n",
        "      #check if returned data contains the file already already\n",
        "\n",
        "\n",
        "\n",
        "      #\"\"CREATE TABLE Laptop (Id int(11) NOT NULL, Name varchar(250) NOT NULL, Price float NOT NULL, Purchase_date Date NOT NULL, PRIMARY KEY (Id)) \"\"\n",
        "      cursor = connection.cursor(buffered=True)\n",
        "      cursor.execute(mySql_Create_Table_Query)\n",
        "      result = cursor.fetchall()\n",
        "      for row in result:\n",
        "        print(row)\n",
        "\n",
        "except Error as e:\n",
        "    print(\"Error while connecting to MySQL\", e)\n",
        "finally:\n",
        "    if connection.is_connected():\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "        print(\"MySQL connection is closed\")\n"
      ],
      "metadata": {
        "id": "IIECm9ONKvEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a66daf-7ee6-43f3-d19c-6e188c6746b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to MySQL Server version  8.0.23-PlanetScale\n",
            "You're connected to database:  ('stock_live_updates',)\n",
            "(1, 'BTCY', 'Biotricity Announces 1-For-6 Reverse Stock Split', '\\xa0Biotricity Holdings, Inc. (\"Biotricity\" or the \"Company\") (NASDAQ:BTCY), a medical diagnostic and consumer healthcare technology company, today it will effect a reverse stock split of its common stock', None, None, None, None)\n",
            "(2, 'AHI', 'Advanced Health Intelligence Announced Implementation of ADS Ratio Change With Effect Of 1:4 Reverse ADS Split', '- Advanced Health Intelligence Ltd (NASDAQ:AHI) (“Advanced Health Intelligence”, “AHI” or the “Company”) announces today that its planned ratio change of the Company’s American', None, None, None, None)\n",
            "(5, 'N/A', 'Accelerate Diagnostics Announces 1-For-10 Reverse Stock Split', 'Accelerate Diagnostics Announces 1-For-10 Reverse Stock Split', None, None, '', '?')\n",
            "(6, 'AAON', 'AAON Adopts 3-For-2 Stock Split', 'AAON Inc\\xa0(NASDAQ: AAON) said the Board of Directors\\xa0', None, None, 'https://finance.yahoo.com/news/aaons-three-two-stock-split-151200501.html', '?')\n",
            "(7, 'OWLT', 'Owlet, Inc. Announces 1-For-14 Reverse Stock Split Will Become Effective Today, July 7, 2023', \"The Company's Class A common stock will begin trading on a split-adjusted basis on July 10, 2023\\n\\nOwlet, Inc.\\xa0(NYSE:OWLT) ('Owlet' or the 'Company')\\xa0today announced that the previously\", None, None, 'https://finance.yahoo.com/news/owlet-inc-announces-1-14-141500802.html', '?')\n",
            "(8, 'ACST', 'Acasti Pharma Announces 1-For-6 Reverse Stock Split', \"Acasti Pharma Inc. ('Acasti' or the 'Company') (NASDAQ:ACST), a late-stage, biopharma company advancing GTX-104, its novel formulation of nimodipine that addresses the high unmet medical needs for a\", None, None, 'https://finance.yahoo.com/news/acasti-pharma-announces-1-6-120000848.html', '?')\n",
            "(9, 'N/A', 'Bright Minds Earlier Announced 1:5 Reverse Stock Split, Effective July 14, 2023', 'Bright Minds Earlier Announced 1:5 Reverse Stock Split, Effective July 14, 2023', None, None, '', '?')\n",
            "(10, 'N/A', 'Jiuzi Holdings Plans 1-For-18 Reverse Share Split', 'Ordinary Shares will begin trading on a reverse share split-adjusted basis at the opening of Nasdaq\\xa0on or around Wednesday, July 10, 2023.\\xa0The reverse share split is intended for the Company to regain', None, None, '', '?')\n",
            "(11, 'AUID', 'authID Announces 1-For-8 Reverse Stock Split Effective Monday July 10, 2023', \"\\xa0authID®\\xa0(NASDAQ:AUID) (the 'Company') a leading provider of innovative biometric identity verification and authentication solutions today announced that effective before market open on July 10,\", None, None, 'https://finance.yahoo.com/news/authid-announces-reverse-stock-split-120000979.html', '?')\n",
            "(12, 'N/A', 'AAON Announces 3-For-2 Stock Split, To Be Paid In The Form Of Stock Dividend', 'AAON Announces 3-For-2 Stock Split, To Be Paid In The Form Of Stock Dividend', None, None, '', '?')\n",
            "(13, 'TLIS', 'Talis Biomedical Announces 1-For-15 Reverse Stock Split', 'Talis Biomedical Corporation (NASDAQ:TLIS), a diagnostic company dedicated to advancing health equity and outcomes through the delivery of accurate infectious disease testing in the moment of need, at the point of care,', None, None, 'https://finance.yahoo.com/news/talis-biomedical-announces-1-15-120000703.html', '?')\n",
            "(14, 'N/A', 'Biofrontera Announces 1-For-20 Reverse Stock Split', 'Biofrontera Announces 1-For-20 Reverse Stock Split', None, None, '', '?')\n",
            "(20, 'N/A', 'Katapult Board Approves 1-For-25 Reverse Stock Split Of Common Stock', \"The Reverse Stock Split was approved by Katapult's stockholders at the Annual Meeting of Stockholders held virtually on June 6, 2023.\\xa0The Reverse Stock Split will become effective at 5:01 p.m. Eastern Time on\", None, None, '', '?')\n",
            "(21, 'N/A', 'Lion Group Stock Trading Premarket Following Effect Of 1:28 Reverse Stock Split', 'Lion Group Stock Trading Premarket Following Effect Of 1:28 Reverse Stock Split', None, None, '', '?')\n",
            "(22, 'N/A', 'Juzi Holdings Shares Up 100%+ Premarket; Stock Began Trading On A 1:18 Reverse Split On July 10', 'Juzi Holdings Shares Up 100%+ Premarket; Stock Began Trading On A 1:18 Reverse Split On July 10', None, None, '', '?')\n",
            "(23, 'N/A', 'EXL Announces Five-for-One Forward Stock Split, Expected To Begin Aug. 2', 'EXL Announces Five-for-One Forward Stock Split, Expected To Begin Aug. 2', None, None, '', '?')\n",
            "(24, 'N/A', 'EXL Announces Five-for-One Forward Stock Split, Expected To Begin Aug. 2', 'EXL Announces Five-for-One Forward Stock Split, Expected To Begin Aug. 2', None, None, '', '?')\n",
            "(25, 'N/A', 'Lion Group Stock Trading Premarket Following Effect Of 1:28 Reverse Stock Split', 'Lion Group Stock Trading Premarket Following Effect Of 1:28 Reverse Stock Split', None, None, '', '?')\n",
            "(26, 'N/A', 'Juzi Holdings Shares Up 100%+ Premarket; Stock Began Trading On A 1:18 Reverse Split On July 10', 'Juzi Holdings Shares Up 100%+ Premarket; Stock Began Trading On A 1:18 Reverse Split On July 10', None, None, '', '?')\n",
            "(27, 'N/A', 'Katapult Board Approves 1-For-25 Reverse Stock Split Of Common Stock', \"The Reverse Stock Split was approved by Katapult's stockholders at the Annual Meeting of Stockholders held virtually on June 6, 2023.\\xa0The Reverse Stock Split will become effective at 5:01 p.m. Eastern Time on\", None, None, '', '?')\n",
            "(28, 'N/A', 'Bright Minds Earlier Announced 1:5 Reverse Stock Split, Effective July 14, 2023', 'Bright Minds Earlier Announced 1:5 Reverse Stock Split, Effective July 14, 2023', None, None, '', '?')\n",
            "(29, 'N/A', 'Accelerate Diagnostics Announces 1-For-10 Reverse Stock Split', 'Accelerate Diagnostics Announces 1-For-10 Reverse Stock Split', None, None, '', '?')\n",
            "(30, 'N/A', 'Quoin Pharmaceuticals Shares Trading Following Effect Of 1:12 Reverse ADR Split', 'Quoin Pharmaceuticals Shares Trading Following Effect Of 1:12 Reverse ADR Split', None, None, '', '?')\n",
            "(31, 'N/A', 'Bluejay Diagnostics Board Approves Reverse Stock Split Of Its Shares At Ratio Of 1-For-20', 'Bluejay Diagnostics Board Approves Reverse Stock Split Of Its Shares At Ratio Of 1-For-20', None, None, '', '?')\n",
            "(32, 'TSX', 'Theratechnologies Plans 1-For-4 Reverse Stock Split', \"The Consolidation will be effective July\\xa031, 2023, subject to applicable regulatory approvals, including the Toronto Stock Exchange (the\\xa0'TSX') and the Nasdaq Stock Market ('NASDAQ').\\xa0\", None, None, 'https://finance.yahoo.com/news/gran-tierra-energy-inc-completes-043000217.html', '?')\n",
            "(33, 'OCX', 'Oncocyte Announces 1-For-20 Reverse Stock Split', \"Oncocyte Corporation\\xa0(NASDAQ:OCX) (the 'Company'), a precision diagnostics company, announced today that the Company will implement a 1-for-20 reverse stock split of the outstanding shares of its common\", None, None, 'https://finance.yahoo.com/news/oncocyte-announces-1-20-reverse-173500100.html', '?')\n",
            "(34, 'ABVC', 'ABVC BioPharma Announces 1-For-10 Reverse Stock Split', \"ABVC BioPharma, Inc.\\xa0(NASDAQ:ABVC) ('Company'), a clinical-stage biopharmaceutical company developing therapeutic solutions in ophthalmology, neurology, and oncology/hematology, today announced a reverse\", None, None, 'https://finance.yahoo.com/news/abvc-biopharma-announces-reverse-stock-163000815.html', '?')\n",
            "(35, 'N/A', 'Rush Enterprises Announces Three-For-Two Stock Split With Respect To Both Class A And Class B Common Stock', 'Rush Enterprises Announces Three-For-Two Stock Split With Respect To Both Class A And Class B Common Stock', None, None, '', '?')\n",
            "(36, 'N/A', 'View Stockholders Approve Reverse Stock Split', 'View Stockholders Approve Reverse Stock Split', None, None, '', '?')\n",
            "(37, 'N/A', \"View's Reverse Stock Split Expected To Become Effective For Trading On July 27, 2023; Board Selects 60-for-1 Reverse Stock Split Ratio\", \"View's Reverse Stock Split Expected To Become Effective For Trading On July 27, 2023; Board Selects 60-for-1 Reverse Stock Split Ratio\", None, None, '', '?')\n",
            "(38, 'N/A', 'Biolase Announces 1-For-100 Reverse Stock Split', 'Biolase Announces 1-For-100 Reverse Stock Split', None, None, '', '?')\n",
            "(39, 'N/A', 'Ontrak Announces 1-For-6 Reverse Split', 'Ontrak Announces 1-For-6 Reverse Split', None, None, '', '?')\n",
            "(40, 'N/A', 'INVO BioScience Announces A 1-For-20 Reverse Stock Split Effective Pre-Market Opening On July 28', 'INVO BioScience Announces A 1-For-20 Reverse Stock Split Effective Pre-Market Opening On July 28', None, None, '', '?')\n",
            "(41, 'UPC', 'Universe Pharmaceuticals INC Announces 6-For-1 Share Consolidation', \"\\xa0Universe Pharmaceuticals INC (NASDAQ:UPC) ('Universe Pharmaceuticals' or the 'Company'), a pharmaceutical producer and distributor in China, today announced that the Company plans to effect a\", None, None, 'https://finance.yahoo.com/sec-filing/LMDX/0000950170-23-016045_1685428/', '?')\n",
            "(43, 'N/A', 'Bruush Oral Care - Announces 1-for-25 Reverse Stock Split Effective Pre-market Opening On August 1, 2023', '-Reuters', None, None, '', '?')\n",
            "(44, 'SMFL', 'Smart for Life Announces 1-For-3 Reverse Stock Split', \"Smart for Life, Inc. (NASDAQ:SMFL) ('Smart for Life' or the 'Company'), Smart for Life, Inc. (NASDAQ:SMFL) ('Smart for Life' or the 'Company'), a high growth global leader in the\", None, None, 'https://finance.yahoo.com/sec-filing/GECC/0001133228-23-004041_1675033', '?')\n",
            "(45, 'MDVL', 'MedAvail Announces 1-For-50 Reverse Stock Split', \"MedAvail Holdings, Inc. (NASDAQ:MDVL) ('MedAvail' or the 'Company'), an innovative pharmacy technology company, announced that it will conduct a reverse stock split of its outstanding shares of common\", None, None, 'https://finance.yahoo.com/news/medavail-announces-1-50-reverse-163000432.html', '?')\n",
            "(46, 'N/A', 'National CineMedia Announces 1-For-10 Reverse Stock Split', 'National CineMedia Announces 1-For-10 Reverse Stock Split', None, None, '', '?')\n",
            "(47, 'N/A', 'Otonomo Technologies Announces 1-For-15 Reverse Stock Split', 'Otonomo Technologies Announces 1-For-15 Reverse Stock Split', None, None, '', '?')\n",
            "(48, 'N/A', 'Altisource Asset Management Corporation Announces a Two-For-One Stock Split', 'Altisource Asset Management Corporation Announces a Two-For-One Stock Split', None, None, '', '?')\n",
            "(49, 'N/A', 'Wheeler Real Estate Investment Trust Board Unanimously Approves One-For-Ten Reverse Stock Split', 'Wheeler Real Estate Investment Trust Board Unanimously Approves One-For-Ten Reverse Stock Split', None, None, '', '?')\n",
            "(50, 'CPRT', 'Copart, Inc. Announces A 2-For-1 Stock Split', 'Copart, Inc. (NASDAQ:CPRT) announced today that its Board of Directors has approved a two-for-one split of its common stock. The stock split will be effected as a stock dividend entitling each stockholder of record to', None, None, '', '?')\n",
            "(51, 'MOBQ', 'Mobiquity Technologies, Inc. Announces 1-For-15 Reverse Stock Split And Extension From Nasdaq Hearings Panel', \"Mobiquity Technologies, Inc. (NASDAQ:MOBQ), a leading provider of next-generation data intelligence and advertising technology solutions, announces that it is implementing a 1-for-15 reverse stock split ('reverse\", None, None, 'https://finance.yahoo.com/sec-filing/MOBQ/0001683168-23-004375_1084267/', '?')\n",
            "(52, 'MULN', 'Mullen Announces A 1-For-9 Reverse Split; Company Also Intends To Begin Repurchasing Up To $25M In Shares Through A Stock Buyback Program', \"\\xa0Mullen Automotive, Inc. (NASDAQ:MULN) ('Mullen' or the 'Company'), an emerging electric vehicle ('EV') manufacturer, today announces a 1 for 9 reverse split. The Company intends to\", None, None, 'https://finance.yahoo.com/news/mullen-automotive-inc-announces-1-165000623.html', '?')\n",
            "(57, 'LTRY', 'Lottery.com Shareholders Overwhelmingly Approve Reverse Stock Split At Annual Meeting; Board Of Directors Approved A Final Ratio Of 20-For-1 For The Reverse Stock Split', \"\\xa0Lottery.com, Inc. (NASDAQ:LTRY, LTRYW)))) ('Lottery.com' or the 'Company') announces the successful conclusion of its 2023 Annual Meeting of Stockholders (the 'Meeting'), held in a\", None, None, 'https://finance.yahoo.com/news/lottery-com-shareholders-overwhelmingly-approve-154300853.html', '?')\n",
            "(58, 'ADIL', \"Adial Pharmaceuticals Announces 1-For-25 Reverse Stock Split To Regain Compliance With Nasdaq's Minimum Bid Price Requirement And Reduce The Public Float\", \"Adial Pharmaceuticals, Inc. (NASDAQ:ADIL, ADILW))))\\xa0('Adial' or the 'Company'), a clinical-stage biopharmaceutical company focused on developing therapies for the treatment and prevention of\", None, None, 'https://finance.yahoo.com/news/adial-pharmaceuticals-announces-reverse-stock-123000176.html', '?')\n",
            "MySQL connection is closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This segment below will delete values in the databsae"
      ],
      "metadata": {
        "id": "L18B92zX0XgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://app.planetscale.com/prichardson0874/stock_live_updates\n",
        "#https://planetscale.com/docs/tutorials/planetscale-quick-start-guide\n",
        "#https://pynative.com/python-mysql-database-connection/\n",
        "#https://stackoverflow.com/questions/54571009/how-to-hide-secret-keys-in-google-colaboratory-from-users-having-the-sharing-lin\n",
        "\n",
        "#Open credentials file and load the JSON variables inside\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/SEC Scraping/Scraping.txt','r') as readfile:\n",
        "  credentials = readfile.read()\n",
        "  credentials = json.loads(credentials)\n",
        "\n",
        "#initiate Connection with the database\n",
        "connection = mysql.connector.connect(\n",
        "  host = credentials[\"HOST\"],\n",
        "  user = credentials[\"USERNAME\"],\n",
        "  passwd= credentials[\"PASSWORD\"],\n",
        "  db = credentials[\"DATABASE\"],\n",
        "  autocommit = True,\n",
        ")\n",
        "\n",
        "#Try connecting and working with the database\n",
        "try:\n",
        "  if connection.is_connected():\n",
        "      db_Info = connection.get_server_info()\n",
        "      print(\"Connected to MySQL Server version \", db_Info)\n",
        "      cursor = connection.cursor()\n",
        "      cursor.execute(\"select database();\")\n",
        "      record = cursor.fetchone()\n",
        "      print(\"You're connected to database: \", record)\n",
        "\n",
        "      ##this section is where the query code will go\n",
        "      mySql_Create_Table_Query = \"\"\"delete FROM benzingaliveupdates WHERE id_number=58\"\"\"\n",
        "\n",
        "      #\"\"CREATE TABLE Laptop (Id int(11) NOT NULL, Name varchar(250) NOT NULL, Price float NOT NULL, Purchase_date Date NOT NULL, PRIMARY KEY (Id)) \"\"\n",
        "      cursor = connection.cursor()\n",
        "      result = cursor.execute(mySql_Create_Table_Query)\n",
        "\n",
        "except Error as e:\n",
        "    print(\"Error while connecting to MySQL\", e)\n",
        "finally:\n",
        "    if connection.is_connected():\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "        print(\"MySQL connection is closed\")"
      ],
      "metadata": {
        "id": "ywjJ_DnOJuxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34444798-08c0-4648-c8e2-7940a9edf9b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to MySQL Server version  8.0.23-PlanetScale\n",
            "You're connected to database:  ('stock_live_updates',)\n",
            "MySQL connection is closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to turn the database call and response into a function passing in the database table name and the SQL function since we will be connecting a few times.  we need a section of code which"
      ],
      "metadata": {
        "id": "fqH5QBY80gPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Access the Sheet for URLs\n",
        "This segment will access the google sheet that I made in order to check each of the URL's for me. This should save a significant amount of time. I am still frustrated that I couldnt get this to work in google sheets natively, but I guess Python is just better in this way."
      ],
      "metadata": {
        "id": "vI3BEAaYf08W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing data from google sheets\n",
        "https://docs.gspread.org/en/v5.7.0/\n",
        "\n",
        "The current work-around would be to use this segment first, then run the analysis segment with a sotred data table"
      ],
      "metadata": {
        "id": "nW930e7NgdK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://docs.gspread.org/en/v5.7.2/user-guide.html#getting-a-cell-value\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "#Open the stockwatch sheet\n",
        "sh = gc.open_by_key('1gL6YuJdplpqQd3H4pHWZGqK6W7YDMkWkGmg3OswNcao')\n",
        "\n",
        "#Open the Rev-split worksheet\n",
        "worksheet = sh.worksheet('Rev-Spl Find')\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "print(rows)\n",
        "\n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "pd.DataFrame.from_records(rows)"
      ],
      "metadata": {
        "id": "uRAwSjSpgfnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access and Write to Database"
      ],
      "metadata": {
        "id": "p-tvdjdyI3Bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Code\n",
        "\n",
        "The code below is the original version of the fetch code that I was able to retrieve results\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2XVUYVue_aM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "#sh = gc.create('A new spreadsheet')\n",
        "\n",
        "# Open our new sheet and add some data.\n",
        "worksheet = gc.open('Stock Watch').sheet1\n",
        "\n",
        "cell_list = worksheet.range('A1:C2')\n",
        "\n",
        "import random\n",
        "for cell in cell_list:\n",
        "  cell.value = random.randint(1, 10)\n",
        "\n",
        "worksheet.update_cells(cell_list)\n",
        "# Go to https://sheets.google.com to see your new spreadsheet."
      ],
      "metadata": {
        "id": "tbBv6WTZfuSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}